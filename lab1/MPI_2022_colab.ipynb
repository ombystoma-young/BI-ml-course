{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ombystoma-young/BI-ml-course/blob/main/lab1/MPI_2022_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVi_jp9DOpL"
      },
      "source": [
        "# MPI (Message-Passing Interface)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "* Mechanism of data exchange between processes\n",
        "* Two types of communication: \n",
        " * **point-to-point**: between two processes \n",
        " * **collective**: between multiple processes\n",
        "* Typically only one program, branching depending on the process \n",
        "* Using the mpi4py Python library \n",
        "\n",
        "An mpi4py tutorial: \n",
        "* https://mpi4py.readthedocs.io/en/stable/tutorial.html\n",
        "\n",
        "\n",
        "Install mpi4py:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgyeyISlEBPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4a508a-05d6-4dd9-edd0-a7010e66042f"
      },
      "source": [
        "!pip install mpi4py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.3.tar.gz (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 8.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.3-cp37-cp37m-linux_x86_64.whl size=2185257 sha256=76a3c725d312a466adbce67cc8e1f4e204395421d5de34eab475fae662600643\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/07/14/6a0c63fa2c6e473c6edc40985b7d89f05c61ff25ee7f0ad9ac\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89pnn-HgDpjV"
      },
      "source": [
        "## A basic example (no data exchange)\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk1x92wcDOpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec923977-54df-4683-f5f3-631eed07795b"
      },
      "source": [
        "# mpi.py\n",
        "from mpi4py import MPI\n",
        "comm = MPI.COMM_WORLD  # ?\n",
        "rank = comm.Get_rank() # index of the current process \n",
        "print (\"hello from process \", rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No communication:  \n",
        "`--allow-run-as-root` for colab, locally dont need it  \n",
        "`-n` number of copies"
      ],
      "metadata": {
        "id": "m7KSz8ssnY4k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgRysQhOEt3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2579b43c-3afb-45c8-abf0-5826c622649c"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 8 python mpi.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello from process  4\n",
            "hello from process  5\n",
            "hello from process  7\n",
            "hello from process  0\n",
            "hello from process  1\n",
            "hello from process  6\n",
            "hello from process  2\n",
            "hello from process  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "! No order between processes.\n",
        "\n",
        "Basic example."
      ],
      "metadata": {
        "id": "P9qC-cucnv3X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UptBtCQoDOqK"
      },
      "source": [
        "## Point-to-point communication of two processes\n",
        "\n",
        "### Example: computing $\\pi$ with MPI (*why this formula*)\n",
        "\n",
        "$$\\pi=\\sqrt{6\\sum_{n=1}^{\\infty}\\frac{1}{n^2}}$$\n",
        "\n",
        "**Exercise:** Theoretically estimate the error resulting if we truncate the series at $N$ terms.  \n",
        "\n",
        "Without MPI:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeSQbxbDOqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab91e96c-e43e-405b-908c-4bd0aebef3f7"
      },
      "source": [
        "import numpy as np\n",
        "a = np.arange(1,200000)\n",
        "print (np.sqrt(6*np.sum(1./(a*a))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1415878789259364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrFHlcHIDOqz"
      },
      "source": [
        "### Functions ``send()``, ``recv()`` (p2p) \n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 2 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NFhjMInDOq4"
      },
      "source": [
        "# Evaluate the sum of 2M terms by splitting into two groups of M terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD \n",
        "rank = comm.Get_rank() \n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "print ('Process', rank, 'found partial sum from term', 1+rank*M, 'to term', 1+(rank+1)*M-1, ': ', s )\n",
        "\n",
        "# process 1 sends its partial sum to process 0\n",
        "if rank == 1:\n",
        "    comm.send(s, dest=0) \n",
        "    \n",
        "# process 0 receives the partial sum from process 1, adds to its own partial sum\n",
        "# and outputs the result    \n",
        "elif rank == 0: \n",
        "    s_other = comm.recv(source=1)\n",
        "    s_total = s+s_other\n",
        "    print ('total partial sum =', s_total)\n",
        "    print ('pi_approx =', np.sqrt(6*s_total))\n",
        "    \n",
        "print ('Process', rank, 'finished')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyaN4fOCFnfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa050cf-1c46-4047-d963-c15fef25c006"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 2 python mpi.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 found partial sum from term 1 to term 100 :  1.6349839001848931\n",
            "Process 1 found partial sum from term 101 to term 200 :  0.004962645830104402\n",
            "Process 1 finished\n",
            "total partial sum = 1.6399465460149976\n",
            "pi_approx = 3.1368263063309683\n",
            "Process 0 finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-wSXWdXH1pY"
      },
      "source": [
        "**Exercise:** Perform the same computation in a \"ping-pong\" manner: one process sums only even terms, the other only odd terms; after adding a new term the process sends the current result to the other process.  \n",
        "\n",
        "## Collective communication (many processes)\n",
        "\n",
        "Perform efficient (fast, load-balanced) collective operations (e.g., summations) involving multiple processes. \n",
        "\n",
        "<img src='https://materials.jeremybejarano.com/MPIwithPython/_images/fastSum.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL5OC2CNDOrY"
      },
      "source": [
        "#from IPython.display import Image\n",
        "#Image(filename=\"fastSum.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5eg3eZdDOr1"
      },
      "source": [
        "### Function ``gather()``\n",
        "\n",
        "Pass data from all processes to the chosen process.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 5 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGNWAOcPDOr-"
      },
      "source": [
        "# Evaluate the sum of MN terms by splitting into M groups of N terms.\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size() # total number of processes\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "M = 100\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "s = getPartialSum(1+rank*M, 1+(rank+1)*M)\n",
        "\n",
        "partialSums = comm.gather(s, root=0)\n",
        "print ('partialSums gathered at process %d:' %(rank), partialSums) \n",
        "\n",
        "if rank == 0:\n",
        "    print ('pi_approx =', np.sqrt(6*np.sum(partialSums)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWUfMyhSGG6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2018df50-e0b8-488e-dd90-ff750e99d901"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 5 python mpi.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "partialSums gathered at process 4: None\n",
            "partialSums gathered at process 1: None\n",
            "partialSums gathered at process 3: None\n",
            "partialSums gathered at process 2: None\n",
            "partialSums gathered at process 0: [1.6349839001848931, 0.004962645830104402, 0.0016597368826256017, 0.0008309063464401552, 0.0004988762708311448]\n",
            "pi_approx = 3.1396841231387222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCXH5iJXICPd"
      },
      "source": [
        "### Function ``bcast()`` (broadcasting)\n",
        "\n",
        "Pass data from the chosen process to all other processes.\n",
        "\n",
        "Save as `mpi.py` and run with `mpirun -n 3 python mpi.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw2GT71eDOsY"
      },
      "source": [
        "# basic usage of bcast()\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "\n",
        "if rank == 0:\n",
        "    some_data = {0: 'abcd', 1:1234}\n",
        "else:\n",
        "    some_data = None\n",
        "    \n",
        "print (\"I'm process\", rank, '; data before broadcasting:', some_data)\n",
        "data = comm.bcast(some_data, root=0)\n",
        "print (\"I'm process\", rank, '; data after broadcasting:', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRlOFLVFGL-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfa1955-e2bb-44e3-e444-8a85384a01c1"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 3 python mpi.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm process 0 ; data before broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data before broadcasting: None\n",
            "I'm process 0 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 1 ; data before broadcasting: None\n",
            "I'm process 1 ; data after broadcasting: {0: 'abcd', 1: 1234}\n",
            "I'm process 2 ; data after broadcasting: {0: 'abcd', 1: 1234}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNm6buMYITTW"
      },
      "source": [
        "### Functions ``scatter()``, ``reduce()``\n",
        "\n",
        "* ``scatter()``: distribute data from one source to all processes\n",
        "* ``reduce()``: combine data from all processes using a collective operation like `sum` or `max` (without order dependence)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTGFw1YLDOsq"
      },
      "source": [
        "# Evaluate the sum of N terms by scattering them to N processes.\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "if rank == 0:\n",
        "    data2scatter = [a*a for a in range(1,size+1)]\n",
        "else:\n",
        "    data2scatter = None\n",
        "    \n",
        "data = comm.scatter(data2scatter, root=0)\n",
        "\n",
        "print ('Data at process', rank, ':', data)\n",
        "\n",
        "b = 1./data\n",
        "\n",
        "partialSum = comm.reduce(b, op = MPI.SUM, root = 0)\n",
        "\n",
        "print ('Partial sum at process', rank, ':', partialSum)\n",
        "\n",
        "if rank == 0:\n",
        "    result = np.sqrt(6*partialSum)\n",
        "    print ('Pi_approx:', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5NWqw3HGRTX"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 8 python mpi.py "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tmm87fhIdwU"
      },
      "source": [
        "## Example: parallel scalar product\n",
        "* Generate two random vectors $\\mathbf x$ and $\\mathbf y$ at the root process. Goal: compute their scalar product $\\langle\\mathbf x,\\mathbf y\\rangle$ \n",
        "* Divide $\\mathbf x$ and $\\mathbf y$ into chunks and scatter them to the other processes\n",
        "* Compute scalar products between chunks at each process\n",
        "* Obtain $\\langle\\mathbf x,\\mathbf y\\rangle$ by reducing (summing) local scalar products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiixH1tCDOs9"
      },
      "source": [
        "#\"to run\" syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #length of vectors\n",
        "\n",
        "#arbitrary example vectors, generated to be evenly divided by the number of\n",
        "#processes for convenience\n",
        "\n",
        "x = np.random.rand(N) if comm.rank == 0 else None\n",
        "y = np.random.rand(N) if comm.rank == 0 else None\n",
        "\n",
        "#initialize as numpy arrays\n",
        "dot = np.array([0.])\n",
        "local_N = np.array([0])\n",
        "\n",
        "#test for conformability\n",
        "if rank == 0:\n",
        "                if (N != y.size):\n",
        "                                print (\"vector length mismatch\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #currently, our program cannot handle sizes that are not evenly divided by\n",
        "                #the number of processors\n",
        "                if (N % size != 0):\n",
        "                                print (\"the number of processors must evenly divide n.\")\n",
        "                                comm.Abort()\n",
        "\n",
        "                #length of each process's portion of the original vector\n",
        "                local_N = np.array([N//size])\n",
        "\n",
        "#communicate local array size to all processes\n",
        "comm.Bcast(local_N, root=0)\n",
        "\n",
        "#initialize as numpy arrays\n",
        "local_x = np.zeros(local_N)\n",
        "local_y = np.zeros(local_N)\n",
        "\n",
        "#divide up vectors\n",
        "comm.Scatter(x, local_x, root=0)\n",
        "comm.Scatter(y, local_y, root=0)\n",
        "\n",
        "#local computation of dot product\n",
        "local_dot = np.array([np.dot(local_x, local_y)])\n",
        "\n",
        "#sum the results of each\n",
        "comm.Reduce(local_dot, dot, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "                print (\"The dot product computed with MPI:\", dot[0])\n",
        "                print (\"The dot product computed w/o  MPI:\", np.dot(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjotqyGTHGfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50781edd-76e6-433a-d284-290e99e28fbc"
      },
      "source": [
        "!mpirun --allow-run-as-root -n 10 python mpi.py 4000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product computed with MPI: 999649.6724805026\n",
            "The dot product computed w/o  MPI: 999649.672480504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcjQGyquDOtQ"
      },
      "source": [
        "\n",
        "**Exercise:** Why is the result $\\approx$ 1E6? A bad RNG?\n",
        "\n",
        "*2Q: Why near 1m, why not exact*  \n",
        "BNL\n",
        "\n",
        "$E[<x, y>]$  \n",
        "$<x, y>/N - 1/4 \\sim O(N^{-1/2})$  \n",
        "цпт $\\sum \\frac{(x_ny_n - 1/4)}{\\sqrt{N}} → Normal\\ sh$\n",
        "\n",
        "### Reduce-based computation of $\\pi$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODozaK9rDOtS"
      },
      "source": [
        "# run syntax example: mpirun -n 10 python mpi.py 4000000\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.Get_rank()\n",
        "size = comm.Get_size()\n",
        "\n",
        "#read from command line\n",
        "N = int(sys.argv[1])    #number of terms\n",
        "\n",
        "#initialize as numpy array\n",
        "s = np.array([0.])\n",
        "\n",
        "#test for conformability\n",
        "if (N % size != 0):\n",
        "    print (\"the number of processors must evenly divide n.\")\n",
        "    comm.Abort()\n",
        "\n",
        "#length of each process's portion of the original vector\n",
        "local_N = np.array([N/size])\n",
        "\n",
        "def getPartialSum(start, end):\n",
        "    a = np.arange(start, end)\n",
        "    return np.sum(1./(a*a))\n",
        "\n",
        "#local computation of partial sum\n",
        "local_s = getPartialSum(1+rank*local_N, 1+(rank+1)*local_N)\n",
        "local_s = np.array([local_s])\n",
        "\n",
        "#sum the results of each local sum\n",
        "comm.Reduce(local_s, s, op = MPI.SUM)\n",
        "\n",
        "if (rank == 0):\n",
        "    pi_approx = np.sqrt(6*s[0])\n",
        "    print (\"pi_approx:\", pi_approx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOuSPxKJDOtl"
      },
      "source": [
        "The program execution time can be measured with commands `time` or `/usr/bin/time -v`:\n",
        "\n",
        "(`real`: wall clock time).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIc3lRQwJFjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d43eb3b-042c-418b-998e-11e2a4b213b6"
      },
      "source": [
        "!time mpirun --allow-run-as-root -n 1 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root -n 2 python mpi.py 100000000\n",
        "!time mpirun --allow-run-as-root -n 4 python mpi.py 100000000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pi_approx: 3.14159264404049\n",
            "\n",
            "real\t0m1.556s\n",
            "user\t0m0.605s\n",
            "sys\t0m0.905s\n",
            "pi_approx: 3.1415926440404958\n",
            "\n",
            "real\t0m1.285s\n",
            "user\t0m1.159s\n",
            "sys\t0m1.054s\n",
            "pi_approx: 3.1415926440404975\n",
            "\n",
            "real\t0m1.673s\n",
            "user\t0m1.801s\n",
            "sys\t0m1.165s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLSL8ssHKvq3"
      },
      "source": [
        "Speedup and efficiency of parallelization with 2 processes: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbMya2FYDOto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604984ad-1a95-49de-b7d5-1956c868bea1"
      },
      "source": [
        "Speedup = 1.821/1.325\n",
        "print ('Speedup:', Speedup)\n",
        "\n",
        "Efficiency = Speedup/2\n",
        "print ('Efficiency:', Efficiency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speedup: 1.3743396226415094\n",
            "Efficiency: 0.6871698113207547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| -------------------------------- | t1  \n",
        "| -------------- |  \n",
        "| -------------- | (n=3)  \n",
        "| -------------- | t3 \n",
        "\n",
        "$$Speedup =\\frac{t_1}{t_3}$$\n",
        "$$Efficiency =\\frac{t_1}{t_3\\cdot n}$$"
      ],
      "metadata": {
        "id": "PqU_xgZSyElp"
      }
    }
  ]
}